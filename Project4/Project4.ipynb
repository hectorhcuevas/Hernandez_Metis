{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports/ Defining Helper Fuctions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests_oauthlib import OAuth1\n",
    "import cnfg\n",
    "import tweepy\n",
    "from tweepy.models import Status, ResultSet\n",
    "import re\n",
    "from datetime import datetime\n",
    "import random\n",
    "import preprocessor as p\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "import gensim\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "from collections import Counter\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "#nlp = spacy.load('en')\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk_stopwords = stopwords.words(\"english\")+[\"health\",'healthy','fitness','food','...','dont','follow','good','new','good','giving','get','like','share','week','pron','please','today','day','monday','sunday',\"week\",\"fit\",\"amp\",'february',\"-pron-\",\"rt\", \"via\",\"-»\",\"--»\",\"--\",\"---\",\"-->\",\"<--\",\"->\",\"<-\",\"«--\",\"«\",\"«-\",\"»\",\"«»\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = cnfg.load(\".twitter_config\")\n",
    "\n",
    "oauth = OAuth1(config[\"consumer_key\"],\n",
    "               config[\"consumer_secret\"],\n",
    "               config[\"access_token\"],\n",
    "               config[\"access_token_secret\"])\n",
    "\n",
    "auth = tweepy.OAuthHandler(config[\"consumer_key\"],\n",
    "                           config[\"consumer_secret\"])\n",
    "#auth = tweepy.AppAuthHandler(config[\"consumer_key\"],\n",
    "#                           config[\"consumer_secret\"])\n",
    "\n",
    "#auth.set_access_token(config[\"access_token\"],\n",
    "#                      config[\"access_token_secret\"])\n",
    "\n",
    "api=tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_twitter(query, number):\n",
    "\t\"\"\"\n",
    "\tArgument Order: query, number\n",
    "\n",
    "\tWill search twitter for the query. Query can be a list.\n",
    "\tNumber relates to how many tweets\n",
    "\n",
    "\tReturns a list of tweets\n",
    "\t\"\"\"\n",
    "\tassert type(query) == str, \"Please enter a query in the form of a string\"\n",
    "\tassert type(number) == int, \"Please enter the number of as an integer\"\n",
    "\n",
    "\treturn list(tweepy.Cursor(api.search, q=query, lang='en', tweet_mode='extended').items(number))\n",
    "\n",
    "def extract_handle(tweet):\n",
    "\t\"\"\"\n",
    "\tArgument Order: tweet\n",
    "\n",
    "\tExtracts the twitter handle for a given tweet. @ symbol not included.\n",
    "\n",
    "\tReturns the handle - string type\n",
    "\t\"\"\"\n",
    "\tassert type(tweet) == Status, \"Please enter in a tweet of type Status\"\n",
    "\n",
    "\treturn tweet.__dict__['user'].screen_name\n",
    "\n",
    "def extract_text(tweet):\n",
    "\t\"\"\"\n",
    "\tArgument Order: tweet\n",
    "\n",
    "\tExtracts the clean text of a tweet. Remove links and emoji's\n",
    "\tReturns clean text of the tweet\n",
    "\t\"\"\"\n",
    "\n",
    "\t#this function can be mapped to a list of tweets (status type)\n",
    "\n",
    "\tassert type(tweet) == Status, \"Please enter in a tweet of type Status\"\n",
    "\n",
    "\tregex = r\"http\\S+\"\n",
    "\tsubset = \"\"\n",
    "\n",
    "\temoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "\tif hasattr(tweet, \"text\"):\n",
    "\t\tclean = re.sub(regex, subset, tweet.text.strip())\n",
    "\t\tclean = emoji_pattern.sub(subset, clean).strip()\n",
    "\telse:\n",
    "\t\tclean = re.sub(regex, subset, tweet.full_text.strip())\n",
    "\treturn clean\n",
    "\n",
    "\n",
    "\n",
    "def extract_hashtags(tweet):\n",
    "\t\"\"\"\n",
    "\tArgument Order: tweet\n",
    "\n",
    "\tReturn a list of hastags present in a given tweet\n",
    "\t\"\"\"\n",
    "\thashtags = []\n",
    "\tassert type(tweet) == Status, \"Please enter in a tweet of type Status\"\n",
    "\n",
    "\tif hasattr(tweet, \"entities\"):\n",
    "\t\tif tweet.entities['hashtags'] == []:\n",
    "\t\t\treturn []\n",
    "\t\telse:\n",
    "\t\t\tfor i in tweet.entities['hashtags']:\n",
    "\t\t\t\thashtags.append(i['text'])\n",
    "\telse:\n",
    "\t\tprint(\"No entity method!\")\n",
    "\treturn hashtags\n",
    "\n",
    "\n",
    "def extract_datetime(tweet):\n",
    "\t\"\"\"\n",
    "\tArgument Order: tweet\n",
    "\n",
    "\tReturns a datetime object\n",
    "\t\"\"\"\n",
    "\tassert type(tweet) == Status, \"Please enter in a tweet of type Status\"\n",
    "\n",
    "\treturn tweet.created_at\n",
    "\n",
    "def extract_users_tweets(handle, number):\n",
    "\t\"\"\"\n",
    "\tArgument Order: handle, number of tweets to extract\n",
    "\t\n",
    "\tExtract's a user's tweets\n",
    "\t\"\"\"\n",
    "\tfinal = ResultSet() #can change to resultset later if I want\n",
    "\n",
    "\ttry:\n",
    "\t\tfor status in tweepy.Cursor(api.user_timeline, screen_name=handle, count=200, include_rts=True).items(number):\n",
    "\t\t\tfinal.append(status)\n",
    "\texcept:\n",
    "\t\tprint(\"{} is a protected user!\")\n",
    "\t\treturn []\n",
    "\n",
    "\treturn final\n",
    "\n",
    "def tweet_cleaner(tweet_text):\n",
    "    clean_tweets = []\n",
    "    for text in tweet_text:\n",
    "        clean_tweets.append(p.clean(text))\n",
    "\n",
    "    import string\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    tweets = []\n",
    "    for text in clean_tweets:\n",
    "        tweets.append((str(text).translate(translator)))  \n",
    "\n",
    "    tweet = []\n",
    "    for words in tweets:\n",
    "        tweet.append(words.lower())\n",
    "\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    final_tweets = []\n",
    "    for words in tweet:\n",
    "        final_tweets.append(wordnet_lemmatizer.lemmatize(words))\n",
    "    return(final_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gathering Tweets/Extracting Features & Cleaning Tweet Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nutrition_tweets.pkl', 'rb') as f:\n",
    "    nutrition_tweets = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('health_tweets.pkl', 'rb') as f:\n",
    "    health_tweets = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = []\n",
    "\n",
    "for tweet in nutrition_tweets:\n",
    "    hashtags.append(extract_hashtags(tweet))\n",
    "\n",
    "tweet_text = []\n",
    "\n",
    "for tweet in nutrition_tweets:\n",
    "    tweet_text.append(extract_text(tweet))\n",
    "    \n",
    "date_time = []\n",
    "\n",
    "for tweet in nutrition_tweets:\n",
    "    date_time.append(extract_datetime(tweet))\n",
    "    \n",
    "handle = []\n",
    "\n",
    "for tweet in nutrition_tweets:\n",
    "    handle.append(extract_handle(tweet))\n",
    "    \n",
    "location = []\n",
    "for tweet in nutrition_tweets:\n",
    "    location.append(tweet.place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags2 = []\n",
    "\n",
    "for tweet in health_tweets:\n",
    "    hashtags2.append(extract_hashtags(tweet))\n",
    "\n",
    "tweet_text2 =[]\n",
    "\n",
    "for tweet in health_tweets:\n",
    "    tweet_text2.append(extract_text(tweet))\n",
    "    \n",
    "date_time2 = []\n",
    "\n",
    "for tweet in health_tweets:\n",
    "    date_time2.append(extract_datetime(tweet))\n",
    "    \n",
    "handle2 = []\n",
    "\n",
    "for tweet in health_tweets:\n",
    "    handle2.append(extract_handle(tweet))\n",
    "    \n",
    "location2 = []\n",
    "for tweet in health_tweets:\n",
    "    location2.append(tweet.place)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Dataframe (if not already) from Extracted Tweets/Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 820\n"
     ]
    }
   ],
   "source": [
    "my_tweets = search_twitter('nutrition',10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"text\"] = tweet_text\n",
    "df[\"datetime\"]= date_time\n",
    "df[\"location\"] = location\n",
    "df[\"hashtags\"] = hashtags\n",
    "df[\"handle\"] = handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame()\n",
    "df2[\"text\"] = tweet_text2\n",
    "df2[\"datetime\"]= date_time2\n",
    "df2[\"location\"] = location2\n",
    "df2[\"hashtags\"] = hashtags2\n",
    "df2[\"handle\"] = handle2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining our two dataframes of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df,df2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_health = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text = df2['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>datetime</th>\n",
       "      <th>location</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>handle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @Notorious_Nava: The #Athletic #MMA #Fitnes...</td>\n",
       "      <td>2018-03-04 02:15:33</td>\n",
       "      <td>None</td>\n",
       "      <td>[Athletic, MMA, Fitness, Judo, Fight, Diet, fi...</td>\n",
       "      <td>BlackTigerSport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @adventureista: Happy #Holi! May the canvas...</td>\n",
       "      <td>2018-03-04 02:15:29</td>\n",
       "      <td>None</td>\n",
       "      <td>[Holi, love, health]</td>\n",
       "      <td>Exluppo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#health #beauty Buy Now: $49.99 6 Pair Andrea ...</td>\n",
       "      <td>2018-03-04 02:15:28</td>\n",
       "      <td>None</td>\n",
       "      <td>[health, beauty]</td>\n",
       "      <td>health_anbeauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Permanent health results are only achieved whe...</td>\n",
       "      <td>2018-03-04 02:15:21</td>\n",
       "      <td>None</td>\n",
       "      <td>[healthyliving, habits, health]</td>\n",
       "      <td>MariStreamlined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @wef: Scientists have created a silicon bea...</td>\n",
       "      <td>2018-03-04 02:15:09</td>\n",
       "      <td>None</td>\n",
       "      <td>[health]</td>\n",
       "      <td>SammKttkrr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text            datetime  \\\n",
       "0  RT @Notorious_Nava: The #Athletic #MMA #Fitnes... 2018-03-04 02:15:33   \n",
       "1  RT @adventureista: Happy #Holi! May the canvas... 2018-03-04 02:15:29   \n",
       "2  #health #beauty Buy Now: $49.99 6 Pair Andrea ... 2018-03-04 02:15:28   \n",
       "3  Permanent health results are only achieved whe... 2018-03-04 02:15:21   \n",
       "4  RT @wef: Scientists have created a silicon bea... 2018-03-04 02:15:09   \n",
       "\n",
       "  location                                           hashtags           handle  \n",
       "0     None  [Athletic, MMA, Fitness, Judo, Fight, Diet, fi...  BlackTigerSport  \n",
       "1     None                               [Holi, love, health]          Exluppo  \n",
       "2     None                                   [health, beauty]  health_anbeauty  \n",
       "3     None                    [healthyliving, habits, health]  MariStreamlined  \n",
       "4     None                                           [health]       SammKttkrr  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = list(map(lambda x: x.replace(\"#\", \"\"), tweet_text))\n",
    "tweet_text = list(map(lambda x: x.replace(\"&amp;\", \"&\"), tweets))\n",
    "tweets = list(map(lambda x: x.replace(\"&gt;\", \"greater than\"), tweet_text))\n",
    "tweet_text = list(map(lambda x: x.replace(\"&lt;\", \"less than\"), tweets))\n",
    "tweets = list(map(lambda x: x.replace(\"&amp;\", \"&\"), tweet_text))\n",
    "tweet_text = list(map(lambda x: x.replace(\"/\", \"\"), tweets))\n",
    "tweets = list(map(lambda x: x.replace(\"...\", \"\"), tweet_text))\n",
    "tweet_text = list(map(lambda x: x.replace(\"\\u200d\", \"\"), tweets))\n",
    "tweets = list(map(lambda x: x.replace(\"\\U0001f986\", \"\"), tweet_text))\n",
    "tweet_text = list(map(lambda x: x.replace(\"\\U0001f942\", \"\"), tweets))\n",
    "tweets = list(map(lambda x: x.replace(\"\\U0001f92f\", \"\"), tweet_text))\n",
    "tweet_text = list(map(lambda x: x.replace(\"\\U0001f911\", \"\"), tweets))\n",
    "tweets = list(map(lambda x: x.replace(\"$\", \"\"), tweet_text))\n",
    "tweet_text = list(map(lambda x: x.replace(\"n't\", \"\"), tweets))\n",
    "tweets = list(map(lambda x: x.replace(\"ç\", \"c\"), tweet_text))\n",
    "tweet_text = list(map(lambda x: x.replace(\"'ll\", \"\"), tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tweets = tweet_cleaner(tweet_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Topic Modeling with LDA/NMF and Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.5, max_features=None, min_df=10,\n",
       "        ngram_range=(1, 2), preprocessor=None,\n",
       "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',...ron-', 'rt', 'via', '-»', '--»', '--', '---', '-->', '<--', '->', '<-', '«--', '«', '«-', '»', '«»'],\n",
       "        strip_accents=None, token_pattern='\\\\b[a-z][a-z]+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(min_df = 10, ngram_range=(1, 2), max_df=.5,\n",
    "                                   stop_words=nltk_stopwords, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\")\n",
    "count_vectorizer.fit(final_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = count_vectorizer.transform(final_tweets).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = dict((v, k) for k, v in count_vectorizer.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = matutils.Sparse2Corpus(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hector/anaconda/lib/python3.6/site-packages/gensim/matutils.py:995: RuntimeWarning: invalid value encountered in subtract\n",
      "  result = psi(alpha) - psi(np.sum(alpha))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-a8a0b32aeca6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLdaModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminimum_probability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.03\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_dir_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[1;32m    745\u001b[0m                         \u001b[0mpass_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_no\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlencorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m                     )\n\u001b[0;32m--> 747\u001b[0;31m                     \u001b[0mgammat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_estep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_alpha\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mdo_estep\u001b[0;34m(self, chunk, state)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollect_sstats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msstats\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumdocs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# avoids calling len(chunk) on a generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[1;32m    521\u001b[0m                 \u001b[0;31m# the update for gamma gives this update. Cf. Lee&Seung 2001.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m                 \u001b[0mgammad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexpElogthetad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcts\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mphinorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpElogbetad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m                 \u001b[0mElogthetad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirichlet_expectation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgammad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m                 \u001b[0mexpElogthetad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mElogthetad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m                 \u001b[0mphinorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpElogthetad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpElogbetad\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/gensim/matutils.py\u001b[0m in \u001b[0;36mdirichlet_expectation\u001b[0;34m(alpha)\u001b[0m\n\u001b[1;32m    993\u001b[0m     \"\"\"\n\u001b[1;32m    994\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 995\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpsi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    996\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpsi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lda = models.LdaModel(corpus=corpus, num_topics=15, id2word=id2word, minimum_probability=.03, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.print_topics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
